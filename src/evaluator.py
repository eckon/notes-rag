import os
import re
import subprocess
import textwrap
from string import Template

from config import CYAN, GREEN, GREY, MAGENTA, RESET, YELLOW

evaluation_prompt = Template(
    textwrap.dedent("""
        You are a helpful assistant.

        Your task is to evaluate the quality of the answer generated by the AI model.
        It does not matter if the expected answer is short and simple or long and complex.
        You just need to evaluate if the generated answer matches the expected answer, even if it is not perfect or more information is provided.
        Use your reasoning to make a decision even if the answer is not directly aligning with the expected answer.
        Some results might still be correct, I will leave that to you to decide.

        The question and answer are the following

        Question: $question
        Answer: $answer

        The AI model has generated the following answer:

        Answer to evaluate: $answer_to_evaluate


        Your task is to compare the answer generated by the AI model with the expected answer.
        The evalation result should always be either true or false, it also needs to be the first line of the response,
        followed by an empty line. The explanation is optional, but would be appreciated.

        Answer with the following format:

        Evaluation Result: <true or false>

        Explanation: <Why you think the answer is correct or incorrect>
    """).strip()
)

qa_pairs: list[tuple[str, str]] = [
    ("What is the name of the user", "Niklas Meyer"),
    ("Can you open the browser?", "Yes"),
    ("Can you access the pinecone vector db for my notes?", "Yes"),
    (
        "What did i do on the '2025-08-17'?",
        textwrap.dedent("""
             Tried out different password manager like doppler and 1password cli.
             Updated my python scripts to use uv as the new package manager.
             Looked into mcp integration for browser to, for example, open big dutchman food website.
             Nothing was done for work as it was on a weekend.
        """).strip(),
    ),
    (
        "I want to delete a camera for bid dutchman what do i need to consider?",
        textwrap.dedent("""
            We want to use the frontend to delete the camera, to trigger all relevant events and cleanup.
            To be able to, we need to update the database filed `automaticRegistrationPending` to `true`.
            Afterwards a user with permission can delete the camera via the frontend.
            For further information see the big dutchman documentation `User Guide for Camera Manager application`.
            Also have a look at my note `camera-manager.md`.
        """).strip(),
    ),
    (
        "What happened over the last week, please note down each day in its own section with bullet points?",
        textwrap.dedent("""
            The content of the returned data does not really matter.
            It is only relevant if we really get only data from the last week,
            and the different tasks in there are from the last week only.
            That the data is divided into the different days and listed as bullet points.
        """).strip(),
    ),
]


def evaluate(model: str, notes_root: str):
    score = 0
    current_dir = os.getcwd()
    expanded_notes_root = os.path.expanduser(notes_root)
    os.chdir(expanded_notes_root)

    for i, (question, answer) in enumerate(qa_pairs):
        print(f"{YELLOW}Question{RESET}:\n{question}\n")
        print(f"{GREEN}Expected Answer{RESET}:\n{answer}\n")

        print(f"{GREY}Generating answer...{RESET}")
        generated_answer = subprocess.run(
            ["opencode", "--model", model, "run", question],
            capture_output=True,
            text=True,
            check=True,
        ).stdout.strip()

        print(f"{MAGENTA}Generated Answer{RESET}:\n{generated_answer}\n")

        print(f"{GREY}Evaluate answer...{RESET}")
        evaluation_result = subprocess.run(
            [
                "opencode",
                "--model",
                model,
                "run",
                evaluation_prompt.substitute(
                    question=question,
                    answer=answer,
                    answer_to_evaluate=generated_answer,
                ),
            ],
            capture_output=True,
            text=True,
            check=True,
        ).stdout.strip()

        print(f"{CYAN}Evaluation{RESET}:\n{evaluation_result}\n")

        # Parse the evaluation result to extract true/false
        match = re.search(
            r"Evaluation Result:\s*(true|false)", evaluation_result, re.IGNORECASE
        )
        is_correct = match and match.group(1).lower() == "true"

        if is_correct:
            score += 1

        print(f"{GREY}Current Score{RESET}: {score}/{i + 1}\n")
        print("------------------------------------------------------------\n")

    print(f"{CYAN}Final Score{RESET}: {score}/{len(qa_pairs)}")

    # Always change back to original directory
    os.chdir(current_dir)


def main():
    model = "anthropic/claude-sonnet-4-20250514"
    notes_root = "~/Documents/notes"

    evaluate(model, notes_root)


if __name__ == "__main__":
    main()
