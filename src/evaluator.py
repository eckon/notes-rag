import argparse
import os
import re
import subprocess
import sys
import textwrap
import time
from string import Template

from config import CYAN, GREEN, GREY, MAGENTA, RED, RESET, YELLOW

evaluation_prompt = Template(
    textwrap.dedent("""
        You are a helpful assistant.

        Your task is to evaluate the quality of the answer generated by the AI model.
        It does not matter if the expected answer is short and simple or long and complex.
        You just need to evaluate if the generated answer matches the expected answer, even if it is not perfect or more information is provided.
        Use your reasoning to make a decision even if the answer is not directly aligning with the expected answer.
        Some results might still be correct, I will leave that to you to decide.

        The question and answer are the following

        Question: $question
        Answer: $answer

        The AI model has generated the following answer:

        Answer to evaluate: $answer_to_evaluate


        Your task is to compare the answer generated by the AI model with the expected answer.
        The evalation result should always be either true or false, it also needs to be the first line of the response,
        followed by an empty line. The explanation is optional, but would be appreciated.

        Answer with the following format:

        Evaluation Result: <true or false>

        Explanation: <Why you think the answer is correct or incorrect>
    """).strip()
)

qa_pairs: list[tuple[str, str]] = [
    ("What is the name of the user", "Niklas Meyer"),
    ("Can you open the browser?", "Yes"),
    ("Can you access the pinecone vector db for my notes?", "Yes"),
    (
        "What did i do on the '2025-08-17'?",
        textwrap.dedent("""
             Tried out different password manager like doppler and 1password cli.
             Updated my python scripts to use uv as the new package manager.
             Looked into mcp integration for browser to, for example, open big dutchman food website.
             Nothing was done for work as it was on a weekend.
        """).strip(),
    ),
    (
        "I want to delete a camera for bid dutchman what do i need to consider?",
        textwrap.dedent("""
            We want to use the frontend to delete the camera, to trigger all relevant events and cleanup.
            To be able to, we need to update the database filed `automaticRegistrationPending` to `true`.
            Afterwards a user with permission can delete the camera via the frontend.
            For further information see the big dutchman documentation `User Guide for Camera Manager application`.
            Also have a look at my note `camera-manager.md`.
        """).strip(),
    ),
    (
        "What happened over the last week, please note down each day in its own section with bullet points?",
        textwrap.dedent("""
            The content of the returned data does not really matter.
            It is only relevant if we really get only data from the last week,
            and the different tasks in there are from the last week only.
            That the data is divided into the different days and listed as bullet points.
        """).strip(),
    ),
]


def evaluate(model: str, notes_root: str, output_file: str | None = None):
    score = 0
    total_questions = len(qa_pairs)
    current_dir = os.getcwd()
    expanded_notes_root = os.path.expanduser(notes_root)

    print(f"Evaluation model: {MAGENTA}{model}{RESET}")
    print(f"Notes directory:  {CYAN}{expanded_notes_root}{RESET}")
    print(f"Total questions:  {YELLOW}{total_questions}{RESET}\n")

    results = []
    start_time = time.time()

    try:
        os.chdir(expanded_notes_root)
    except FileNotFoundError:
        print(f"{RED}Error: Notes directory '{expanded_notes_root}' not found{RESET}")
        return

    for i, (question, answer) in enumerate(qa_pairs):
        print(
            f"{YELLOW}{i + 1}. Question [{i + 1}/{total_questions}]{RESET}:\n{question}\n"
        )
        print(f"{GREEN}Expected Answer{RESET}:\n{answer}\n")

        try:
            print(f"{GREY}Generating answer...{RESET}")
            generated_answer = run_opencode(question, model)

            print(f"{MAGENTA}Generated Answer{RESET}:\n{generated_answer}\n")

            print(f"{GREY}Evaluating answer...{RESET}")
            evaluation_result = run_opencode(
                evaluation_prompt.substitute(
                    question=question,
                    answer=answer,
                    answer_to_evaluate=generated_answer,
                ),
                model,
            )

            print(f"{CYAN}Evaluation{RESET}:\n{evaluation_result}\n")

            # Parse the evaluation result to extract true/false
            match = re.search(
                r"Evaluation Result:\s*(true|false)", evaluation_result, re.IGNORECASE
            )
            is_correct = match and match.group(1).lower() == "true"

            if is_correct:
                score += 1
                status = f"{GREEN}✓ PASS{RESET}"
            else:
                status = f"{RED}✗ FAIL{RESET}"

            print(
                f"{status} - Current Score: {GREEN}{score}/{i + 1} ({score / (i + 1) * 100:.1f}%){RESET}\n"
            )

            # Store result for potential file output
            results.append(
                {
                    "question": question,
                    "expected": answer,
                    "generated": generated_answer,
                    "evaluation": evaluation_result,
                    "correct": is_correct,
                }
            )

        except Exception as e:
            print(f"{RED}Error processing question {i + 1}: {e}{RESET}\n")
            results.append(
                {
                    "question": question,
                    "expected": answer,
                    "generated": f"ERROR: {e}",
                    "evaluation": "ERROR",
                    "correct": False,
                }
            )

    # Always change back to original directory
    os.chdir(current_dir)

    # Final statistics
    end_time = time.time()
    duration = end_time - start_time
    percentage = (score / total_questions) * 100

    print(f"{CYAN}EVALUATION COMPLETE{RESET}")
    print(f"Final Score:  {GREEN}{score}/{total_questions} ({percentage:.1f}%){RESET}")
    print(f"Duration:     {YELLOW}{duration:.1f} seconds{RESET}")
    print(f"Average time: {YELLOW}{duration / total_questions:.1f} seconds{RESET}")

    if output_file:
        save_results_to_file(
            results, output_file, model, score, total_questions, duration
        )


def run_opencode(prompt: str, model: str) -> str:
    command = ["opencode", "--model", model, "run", prompt]
    try:
        result = subprocess.run(
            command,
            capture_output=True,
            text=True,
            check=True,
        )
        return result.stdout.strip()
    except subprocess.CalledProcessError as e:
        error_msg = e.stderr.strip() if e.stderr else "Unknown error"
        raise Exception(f"Command failed with exit code {e.returncode}: {error_msg}")
    except FileNotFoundError:
        raise Exception(
            "opencode command not found. Make sure it's installed and in your PATH."
        )


def save_results_to_file(
    results: list, filename: str, model: str, score: int, total: int, duration: float
):
    try:
        with open(filename, "w") as f:
            f.write("Evaluation Results\n")
            f.write(f"Model: {model}\n")
            f.write(f"Score: {score}/{total} ({score / total * 100:.1f}%)\n")
            f.write(f"Duration: {duration:.1f} seconds\n")
            f.write(f"Timestamp: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write("=" * 80 + "\n\n")

            for i, result in enumerate(results, 1):
                f.write(f"Question {i}:\n{result['question']}\n\n")
                f.write(f"Expected:\n{result['expected']}\n\n")
                f.write(f"Generated:\n{result['generated']}\n\n")
                f.write(f"Evaluation:\n{result['evaluation']}\n\n")
                f.write(f"Result: {'PASS' if result['correct'] else 'FAIL'}\n")
                f.write("-" * 80 + "\n\n")

        print(f"{GREEN}Results saved to {filename}{RESET}")
    except Exception as e:
        print(f"{RED}Error saving results to file: {e}{RESET}")


def main():
    parser = argparse.ArgumentParser(
        description="Evaluate AI model performance on Q&A tasks"
    )
    parser.add_argument(
        "--model",
        default="anthropic/claude-sonnet-4-20250514",
        help="AI model to use for evaluation (default: anthropic/claude-sonnet-4-20250514)",
    )
    parser.add_argument(
        "--notes-root",
        default="~/Documents/notes",
        help="Root directory of notes (default: ~/Documents/notes)",
    )
    parser.add_argument("--output", help="Save detailed results to file")

    args = parser.parse_args()

    try:
        evaluate(args.model, args.notes_root, args.output)
    except KeyboardInterrupt:
        print(f"\n{YELLOW}Operation cancelled{RESET}")
        sys.exit(1)


if __name__ == "__main__":
    main()
